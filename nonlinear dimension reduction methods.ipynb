{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edac027f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This is code for the development of an algorithm for Isomaps and Local Linear Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d4fe7ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "527b1795",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# What is an Isomap? We can learn from this link: https://towardsdatascience.com/what-is-isomap-6e4c1d706b54\n",
    "# or this paper: http://syllabus.cs.manchester.ac.uk/pgt/2020/COMP61021/reference/supervised-isomap.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9c19b14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# What components do we need to run an Isomap function?\n",
    "# The paper informs us we need \n",
    "# 1: Construct a Neighbourhood graph and set the edge lengths equal to a distance metric d(x_i,x_j)\n",
    "# 2: Compute shortest graph s.t. \\text{d_{G}(x_i,x_j)=d(x_i,x_j) \\iff x_i,x_j are linked}\n",
    "# so, the matrix of final values $D_G=\\{d_G(x_i,x_j)\\}$ contains the shortest path distances between all points\n",
    "# 3: Construct d-dimensional embedding: Let \\lambda_p be p^{th} eigenvalue of matrix \\tau(DG) where \\tau(D)=-\\frac{HSH}{2}\n",
    "# the only free parameter in this entire algortihm is actually \\epsilon, oder \"K\" for KNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38775327",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# So, let's begin with the construction of a function for Isomap! Let's name it... Isomap()!\n",
    "def Isomap(datamatrix,number_n_components,KNN_components):\n",
    "    datamatrix = distance_mat(datamatrix, KNN_components)\n",
    "    from sklearn.utils.graph import graph_shortest_path\n",
    "    graph = graph_shortest_path(data, directed=False)\n",
    "    graph = -0.5 * (graph ** 2)\n",
    "    return mds(graph, n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c611b27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now, we need to create a distance matrix. This will take in a datamatrix X, and we will specify the free variable K=5\n",
    "def distance_matrix(X, n_neighbors=5):\n",
    "    def distance(a, b):\n",
    "        return np.sqrt(sum((a - b)**2))\n",
    "\n",
    "# The above code is easy to read, as it follows from definition of distance function in topology and Euclidian l^2 norm\n",
    "# So, we need to implement this code as a full distance matrix, s.t. for all points in the dataset we have an entire matrix\n",
    "# This is often a big matrix\n",
    "    totaldistances = np.array([[dist(point1, point2) for point2 in X] for point1 in X])\n",
    "    neighborpoints = np.zeros_like(totaldistances)\n",
    "    sort_distances = np.argsort(totaldistances, axis=1)[:, 1:n_neighbors+1]\n",
    "    for k,i in enumerate(sort_totaldistances):\n",
    "        neighborpoints[k,i] = totaldistances[k,i]\n",
    "    return neighborpoints, sort_totaldistances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64f07e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# However, note that we must now implement the element of Multidimensional Scaling, or MDS\n",
    "# MDS requires that we center our distancematrix, D\n",
    "# This does not change any of the distances between the pairs of points\n",
    "# However, it does allow for the resulting matrix to have useful properties! :D \n",
    "# Note, K is a matrix of shape m x m, so this tells us it is square\n",
    "\n",
    "# Let's begin\n",
    "# For full parsing of my code here, please refer to our written paper, hosted on github :) \n",
    "\n",
    "def CenterMatrix(K):\n",
    "    number_of_samples = K.shape[0]\n",
    "    Avg_for_Col = np.sum(K, axis=0)/ number_of_samples\n",
    "    Avg_for_Row = (np.sum(K, axis=0)/ number_of_samples)[:, np.newaxis]\n",
    "    Avg_for_entire_matrix = avg_for_Col.sum()/ number_of_samples\n",
    "    #s.t.\n",
    "    K -= Avg_for_Col\n",
    "    K -= Avg_for_Row\n",
    "    K += Avg_for_entire_matrix\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b08c6de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's focus on Linear Algebra :D my favorite topic ever\n",
    "# Remember, definition for Eigenvector is given by {\\displaystyle T(\\mathbf {v} )=\\lambda \\mathbf {v} ,}\n",
    "# where \\lambda is a scalar in a field $F$, and is a \"characteristic value\" oder eigenvalue\n",
    "# If V is a finite-dimensional vector space, we know that\n",
    "# $Au=\\lambda u$ where A is a matrix representation for T and u is the coordinate vector of v\n",
    "\n",
    "# We now need to construct the function to compute the MDS and the corresponding eigenvectors \\& eigenvalues\n",
    "\n",
    "def MultidimensionalScaling(datamatrix, number): \n",
    "    CenterMatrix(datamatrix)\n",
    "    Eigenvalue,Eigenvector = np.linalg.eig(datamatrix)\n",
    "    Eigenpairs =[\n",
    "        (np.abs(Eigenvalue[i]), Eigenvector[:,i]) for i in range(len(Eigenvalue))\n",
    "    ]\n",
    "    \n",
    "# So what's the above code do? Well, we made a function which is basically PCA,\n",
    "# where we're given a Matrix in the form of n x n (Hence, square matrix)\n",
    "# and I specify in the number parameter, how many components I want to use \n",
    "# We than have created a Tuple of items(2), and so we pick the 2 principal components we want, by specifying \n",
    "# the eigenvectors which CONTAIN the two largest eigenvalues, thus we now need to write code to obtain the \n",
    "# transform matrix!\n",
    "\n",
    "    Eigenpairs.sort(key=lambda x:x[0], reverse=True)\n",
    "    Eigenpairs = np.array(Eigenpairs)\n",
    "    W= np.hstack(\n",
    "        [Eigenpairs[i,1].reshape(datamatrix.shape[1],1) for i in range(number)]\n",
    "    )\n",
    "    return W\n",
    "# So, the code above is now creating a new rule(hence the Lambda) where we obtain the\n",
    "# eigenvectors with largest eigenvalues, and obtain a matrix W s.t. W is a subspace transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b398e35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
