{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8904b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we are going to load the dataset! Here, I am going to be using\n",
    "# the MNIST dataset. Scikit does us a favour of preparing testing & training data.\n",
    "from sklearn.datasets import load_digits\n",
    "digitdata = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0666efdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea21828d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we run this code, we can see that the data is filed as an Matrix using numpy!\n",
    "type(digitdata.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f243d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# However, this does not tell us the whole story. Let's look closer at the matrix, to see how high dimensional our data is!\n",
    "(digitdata.data.shape, digitdata.target.shape, digitdata.images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cef423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the above, we can see that our dataset contains 1797 images, each 8x8 in dimension and 1797 labels.\n",
    "# So, each picture can be categorized as a 64 x 1 vector! The labels allow for us to classify each image. \n",
    "# Now, we can begin working on the data. \n",
    "# The code below will display some of the data for visualization!\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(20,4))\n",
    "for index, (image, label) in enumerate(zip(digitdata.data[0:5], \n",
    "                                           digitdata.target[0:5])):\n",
    "    plt.subplot(1, 5, index + 1)\n",
    "    plt.imshow(np.reshape(image, (8,8)), cmap=plt.cm.gray)\n",
    "    plt.title('Training: %i\\n' % label, fontsize = 20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79348ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we can see from above, we have shown 5 training images, showing 5 different digits from the dataset!\n",
    "# Seeing as this is grayscale images, each pixel takes on some value between 0 and 225. \n",
    "# We can now begin to split these digits into both training and test data. \n",
    "# To do so, I again rely on the predefined test and training data given by Scikit!\n",
    "# So,\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(digitdata.data, \n",
    "                                                    digitdata.target,\n",
    "                                                   test_size=0.25,\n",
    "                                                   random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086c8cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will do the same procedure as before, by printing out the Size and Shape of both testing and training data\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7304c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So from above, we see that each image is still categorized as a 64x1 vector. The training data contains 1347 sample\n",
    "# testing contains 450 sample\n",
    "# For the purpose of this project, I am going to be testing logistic regression on this data, before and after\n",
    "# dimension reduction methods. My hypothesis is that we will expect normal logistic regression to work best.\n",
    "# Let's import the logistic regression from scikit!\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(fit_intercept=True,\n",
    "                        multi_class='auto',\n",
    "                        penalty='l2', #ridge regression, for more info please look at \"Introduction to Statistical learning with R\"\n",
    "                        solver='saga',\n",
    "                        max_iter=10000,\n",
    "                        C=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27680705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's print out the result of the above code, to see how exactly this logistic regression will work\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391a9209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, lets test the model and see if the classifier detects the true number of labels. The answer should be 10!\n",
    "clf.fit(X_train, y_train)\n",
    "clf.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ebfc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So, we have 10 labels successfully made. The shape of each label should still be searching for a 64x1 vector\n",
    "# which is the image in vector form. This should look like (10,64) due to 10 labels, 64 pixel image\n",
    "clf.coef_.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab687aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can examine the weights closer for inspection. Let's look at the number 9 \n",
    "clf.coef_[9].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb95fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's examine the same, except for all classes!\n",
    "clf.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf9cad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can view how long this process took, by calculating the number of iterations the software performed to reach\n",
    "# the tolerance! \n",
    "clf.n_iter_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce61b7c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# So, we can now view these coefficients as an image, if we'd like!\n",
    "coef = clf.coef_.copy()\n",
    "plt.imshow(coef[0].reshape(8,8).round(2))\n",
    "# However, this doesn't really capture anything for every class\n",
    "# So, we can create a picture for the coefficients of each class, 10 in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86f9170",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = clf.coef_.copy()\n",
    "scale = np.abs(coef).max()\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "for i in range(10): # 0-9\n",
    "    coef_plot = plt.subplot(3, 4, i + 1) # 2x5 plot\n",
    "\n",
    "    coef_plot.imshow(coef[i].reshape(8,8), \n",
    "                     cmap=plt.cm.RdBu,\n",
    "                     vmin=-scale, vmax=scale,\n",
    "                    interpolation='bilinear')\n",
    "    \n",
    "    coef_plot.set_xticks(()); coef_plot.set_yticks(())\n",
    "    coef_plot.set_xlabel(f'Class {i}')\n",
    "\n",
    "plt.suptitle('Coefficients for various classes');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26ec13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's focus on the prediction of the MNIST data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fc752e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can do so by comparing the unknown against the truth!\n",
    "print(clf.predict(X_test[0:9]))\n",
    "print(y_test[0:9])\n",
    "# We also need to score against the training and testing data\n",
    "# So, \n",
    "clf.score(X_train, y_train) # training score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e16a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = clf.score(X_test, y_test) # test score\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167a7abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So, we have 100% accuracy for the training data, and yet 95.5% for the testing score!\n",
    "# not bad!\n",
    "# Now, we begin on the Confusion matrix. \n",
    "# For reference, please visit https://en.wikipedia.org/wiki/Confusion_matrix\n",
    "# We do this by using scikit package \n",
    "from sklearn import metrics\n",
    "PredictionAccuracy = clf.predict(X_test)\n",
    "\n",
    "cm = metrics.confusion_matrix(y_true=y_test, \n",
    "                         y_pred = PredictionAccuracy, \n",
    "                        labels = clf.classes_)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae02510a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above diagram does not do our data a good representation\n",
    "# So, let's import seaborn! This is a nice package for innovation and heatmaps\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(cm, annot=True, \n",
    "            linewidths=.5, square = True, cmap = 'YlOrRd');\n",
    "\n",
    "plt.ylabel('Correct label')\n",
    "plt.xlabel('Predicted label')\n",
    "all_sample_title = 'Model Accuracy Score: {0}'.format(score)\n",
    "plt.title(all_sample_title);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbf0504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's focus on the digits which were misclassified!\n",
    "index = 0\n",
    "misclassified_images = []\n",
    "for label, predict in zip(y_test, PredictionAccuracy):\n",
    "    if label != predict: \n",
    "        misclassified_images.append(index)\n",
    "    index +=1\n",
    "print(misclassified_images)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.suptitle('Misclassifications');\n",
    "\n",
    "for plot_index, bad_index in enumerate(misclassified_images[0:20]):\n",
    "    p = plt.subplot(4,5, plot_index+1)\n",
    "    \n",
    "    p.imshow(X_test[bad_index].reshape(8,8), cmap=plt.cm.gray,\n",
    "            interpolation='bilinear')\n",
    "    p.set_xticks(()); p.set_yticks(())\n",
    "    \n",
    "    p.set_title(f'Pred: {PredictionAccuracy[bad_index]}, Actual: {y_test[bad_index]}');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0936e2a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
